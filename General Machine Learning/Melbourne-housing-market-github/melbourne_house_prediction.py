# -*- coding: utf-8 -*-
"""Melbourne_House_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pvhm_06K3W6eBxMs6ejKO0cShGpMgFzG
"""

# Commented out IPython magic to ensure Python compatibility.
#import some necessary librairies

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
# %matplotlib inline
import matplotlib.pyplot as plt  # Matlab-style plotting
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)
from sklearn.model_selection import train_test_split

from scipy import stats
from scipy.stats import norm, skew #for some statistics


pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()



from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import metrics

"""## Load Data"""

df = pd.read_csv('/content/cleaned_melbourn_housing_data.csv')

df.columns

a = np.array(df.Price) # First need to convert to array 
house_all1 = a.reshape(-1,1) # Then reshape the data

df1 = pd.DataFrame(scaler.fit_transform(house_all1),columns  = ['Price'])
df = df.drop('Price',axis = 1)
df = pd.concat([df,df1],axis = 1)

df

df.isnull().sum()

df

imp = SimpleImputer(strategy = 'mean')

#Bathroom
imp.fit(df[['Bathroom']])
df['Bathroom'] = imp.transform(df[['Bathroom']]).ravel()
#Car
imp.fit(df[['Car']])
df['Car'] = imp.transform(df[['Car']]).ravel()
#Landsize
imp.fit(df[['Landsize']])
df['Landsize'] = imp.transform(df[['Landsize']]).ravel()
df.isnull().sum()

str(df.loc[1,['Address']].values)

import re
a = re.split('\s', str(df.loc[1,['Address']].values))

b = a[1:]
type(b)

c = ' '.join(b)
c

def street_name(addr):
    a = re.split('\s', addr)
    b = a[1:]
    c = ' '.join(b)
    return c

df['Street'] = df['Address'].apply(street_name)
df['Street'].tail(20)

df_cleaned = df.copy()
df_cleaned['Street_Suburb'] = df_cleaned['Street']+'-'+df_cleaned['Suburb']
df_cleaned['Street_Suburb'].head(10)

categories = ['Postcode', 'Street_Suburb','Type', 'CouncilArea', 'Method','Suburb', 'SellerG', 'Regionname']

for i in categories:
    df_cleaned[i+'_code'] = df_cleaned[i].astype('category').cat.codes


df_cleaned.head(10)

df_train = df_cleaned.drop(categories, axis = 1)
df_train = df_train.drop(['Address', 'Street'], axis = 1)
df_train['Date'] = df_train['Date'].apply(pd.Timestamp)

#Converting Dates as days from 1st of January 2016 and dropping Date column
df_train['DaysFromJan2016']= df_train['Date']-pd.Timestamp('01-01-2016')
df_train['DaysFromJan2016']= df_train['DaysFromJan2016'].dt.days
df_train = df_train.drop('Date', axis = 1)
df_train.head()

X_train, X_test, y_train, y_test = train_test_split(df_train.drop('Price', axis = 1), df_train['Price'], test_size = 0.3, random_state = 100)

"""<h1 id="Model-Building">Model Building<a class="anchor-link" href="#Model-Building">¶</a></h1><hr/>
<p><img src="https://slideplayer.com/slide/15204316/92/images/1/What+is+Regression+Analysis.jpg"/></p>

##1. Xgboost

#### For train eval
"""

from xgboost.sklearn import XGBRegressor
model = XGBRegressor(objective="reg:linear", random_state=42) 
t0  = time.time()
model.fit(X_train, y_train)
print ("fitting time:", round(time.time()-t0, 3), "s")

y_pred = model.predict(X_test)

mse=metrics.mean_squared_error(y_test, y_pred)
mse

"""#### For test eval"""

y_pred = model.predict(X_train)

mse=metrics.mean_squared_error(y_train, y_pred)
mse

"""<h2 id="2.-Gradient-Boosting-Regression-model">2. Gradient Boosting Regression model<a class="anchor-link" href="#2.-Gradient-Boosting-Regression-model">¶</a></h2><hr/>

#### For train mse
"""

import time
# Initiate max R^2 score
max_r2 = 0

# Create Gradient Boosting Regression model that iterates through learning rates 
for i in np.linspace(0.1, 1, 50):
    
    # Initiate model for learning rate i
    gbr = GradientBoostingRegressor(learning_rate = i)
    t0=time.time()
    gbr.fit(X_train, y_train)
    
    
    # Make prediction
    t1=time.time()
    y_pred = gbr.predict(X_test)
   
    
    # Return values for corresponding learning rate
    print ('For learning rate i: %0.2f' %i)
    print('Gradient Boosting Regression MAE: %0.5f'%metrics.mean_absolute_error(y_test,y_pred))
    print('Gradient Boosting MSE:%0.5f'%metrics.mean_squared_error(y_test,y_pred))
    print('Gradient Boosting RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_test,y_pred)))
    print('Gradient Boosting R^2: %0.5f' %metrics.explained_variance_score(y_test,y_pred))
    print ("fitting time:", round(time.time()-t0, 3), "s")
    print ("predict time:", round(time.time()-t1, 3), "s")
    print ('---------------------------------')

    # If R^2 new maximum score, save score and the learning rate
    if metrics.explained_variance_score(y_test,y_pred) > max_r2:
        max_r2 = metrics.explained_variance_score(y_test,y_pred)
        max_i = i
        time1 = round(time.time()-t0, 3)
        y_pred_gbr = y_pred
        
        # Store Standard Error
        se_gbr = stats.sem(y_pred_gbr)

# Print maximum R^2 score and corresponding learning rate
print ('Max R^2 is: %0.5f' %max_r2, 'with learning rate: %0.2f' %max_i,'and fitting time with %0.3f s'%time1)

"""#### For test mse"""

import time
# Initiate max R^2 score
max_r2 = 0

# Create Gradient Boosting Regression model that iterates through learning rates 
for i in np.linspace(0.1, 1, 50): 
    # Make prediction
    t1=time.time()
    y_pred = gbr.predict(X_train)
   
    
    # Return values for corresponding learning rate
    print ('For learning rate i: %0.2f' %i)
    print('Gradient Boosting Regression MAE: %0.5f'%metrics.mean_absolute_error(y_train,y_pred))
    print('Gradient Boosting MSE:%0.5f'%metrics.mean_squared_error(y_train,y_pred))
    print('Gradient Boosting RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_train,y_pred)))
    print('Gradient Boosting R^2: %0.5f' %metrics.explained_variance_score(y_train,y_pred))
    print ("predict time:", round(time.time()-t1, 3), "s")
    print ('---------------------------------')

    # If R^2 new maximum score, save score and the learning rate
    if metrics.explained_variance_score(y_train,y_pred) > max_r2:
        max_r2 = metrics.explained_variance_score(y_train,y_pred)
        max_i = i
        y_pred_gbr = y_pred
        
        # Store Standard Error
        se_gbr = stats.sem(y_pred_gbr)

# Print maximum R^2 score and corresponding learning rate
print ('Max R^2 is: %0.5f' %max_r2, 'with learning rate: %0.2f' %max_i)

"""<h2 id="3.-Linear-Regression-model">3. Linear Regression model<a class="anchor-link" href="#3.-Linear-Regression-model">¶</a></h2><hr/>

#### For train mse
"""

from sklearn.linear_model import LinearRegression
# Initialise Linear Regression model
lr = LinearRegression()
t0=time.time()
lr.fit(X_train, y_train)

# Make Prediction
t1=time.time()
y_pred_lr = lr.predict(X_test)

# Return Results
print('Linear Regression MAE: %0.5f'%metrics.mean_absolute_error(y_test,y_pred_lr))
print('Linear Regression MSE:%0.5f'%metrics.mean_squared_error(y_test,y_pred_lr))
print('Linear Regression RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_test,y_pred_lr)))
print('Linear Regression R^2: %0.5f' %metrics.explained_variance_score(y_test,y_pred_lr))
print ("fitting time:", round(time.time()-t0, 3), "s")
print ("predict time:", round(time.time()-t1, 3), "s")

# Store Standard Error
se_lr = stats.sem(y_pred_lr)

"""#### For test mse"""

# Make Prediction
t1=time.time()
y_pred_lr = lr.predict(X_train)

# Return Results
print('Linear Regression MAE: %0.5f'%metrics.mean_absolute_error(y_train,y_pred_lr))
print('Linear Regression MSE:%0.5f'%metrics.mean_squared_error(y_train,y_pred_lr))
print('Linear Regression RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_train,y_pred_lr)))
print('Linear Regression R^2: %0.5f' %metrics.explained_variance_score(y_train,y_pred_lr))
print ("predict time:", round(time.time()-t1, 3), "s")

# Store Standard Error
se_lr = stats.sem(y_pred_lr)

"""<h2 id="4.-Lasso-Regression-model">4. Lasso Regression model<a class="anchor-link" href="#4.-Lasso-Regression-model">¶</a></h2><hr/>

#### For Train mse
"""

# Initialise Lasso Regression model
lcv = LassoCV()
t0=time.time()
lcv.fit(X_train, y_train)

# Make Prediction
t1=time.time()
y_pred_lcv = lcv.predict(X_test)

# Return Results
print('Lasso Regression MAE: %0.5f'%metrics.mean_absolute_error(y_test,y_pred_lcv))
print('Lasso Regression MSE:%0.5f'%metrics.mean_squared_error(y_test,y_pred_lcv))
print('Lasso Regression RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_test,y_pred_lcv)))
print('Lasso Regression R^2: %0.5f' %metrics.explained_variance_score(y_test,y_pred_lcv))
print ("fitting time:", round(time.time()-t0, 3), "s")
print ("predict time:", round(time.time()-t1, 3), "s")

se_lcv = stats.sem(y_pred_lcv)

"""#### For test mse"""

# Make Prediction
t1=time.time()
y_pred_lcv = lcv.predict(X_train)

# Return Results
print('Lasso Regression MAE: %0.5f'%metrics.mean_absolute_error(y_train,y_pred_lcv))
print('Lasso Regression MSE:%0.5f'%metrics.mean_squared_error(y_train,y_pred_lcv))
print('Lasso Regression RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_train,y_pred_lcv)))
print('Lasso Regression R^2: %0.5f' %metrics.explained_variance_score(y_train,y_pred_lcv))
print ("fitting time:", round(time.time()-t0, 3), "s")
print ("predict time:", round(time.time()-t1, 3), "s")

se_lcv = stats.sem(y_pred_lcv)

"""<h2 id="5.-Random-Forest-Model">5. Random Forest Model<a class="anchor-link" href="#5.-Random-Forest-Model">¶</a></h2><hr/>

#### For train mse
"""

# Initialise Max R^2 variable 
max_r2 = 0

# Create Random Forest Model that iterates between 64 --> 128 trees
for n_trees in range (119, 129):
    hi
    # Initiate model for value n_tree
    rfr = RandomForestRegressor(n_estimators=n_trees, n_jobs=-1)
    t0=time.time()
    rfr.fit(X_train, y_train)
    
    # Make prediction for n_tree sized model
    t1=time.time()
    y_pred = rfr.predict(X_test)
    
    # Store Standard Error
    rfr_sem = stats.sem (y_pred)
    
    # Print Results
    print('For a Random Forest with', n_trees, 'trees in total:')
    print('MAE: %0.5f'%metrics.mean_absolute_error(y_test,y_pred))
    print('MSE:%0.5f'%metrics.mean_squared_error(y_test,y_pred))
    print('RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_test,y_pred)))
    print('R^2: %0.5f' %metrics.explained_variance_score(y_test,y_pred))
    print ("fitting time:", round(time.time()-t0, 3), "s")
    print ("predict time:", round(time.time()-t1, 3), "s")
    print('--------------------------------------')
    
    # If new R^2 the max, store it for reference
    if metrics.explained_variance_score(y_test,y_pred) > max_r2:
        max_r2 = metrics.explained_variance_score(y_test,y_pred)
        max_n_trees = n_trees
        max_rfr_sem = rfr_sem
        y_pred_rfr= y_pred
        
        # Store Standard Error
        se_rfr = stats.sem(y_pred_rfr)

# Return max R^2 and corresponding amount of trees in forest
print ('Max R^2 is: %0.5f' %max_r2, 'at', max_n_trees, 'trees')

"""#### For test mse"""

# Initialise Max R^2 variable 
max_r2 = 0

# Create Random Forest Model that iterates between 64 --> 128 trees
for n_trees in range (119, 129):
    
    # Make prediction for n_tree sized model
    t1=time.time()
    y_pred = rfr.predict(X_train)
    
    # Store Standard Error
    rfr_sem = stats.sem (y_pred)
    
    # Print Results
    print('For a Random Forest with', n_trees, 'trees in total:')
    print('MAE: %0.5f'%metrics.mean_absolute_error(y_train,y_pred))
    print('MSE:%0.5f'%metrics.mean_squared_error(y_train,y_pred))
    print('RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_train,y_pred)))
    print('R^2: %0.5f' %metrics.explained_variance_score(y_train,y_pred))
    print ("predict time:", round(time.time()-t1, 3), "s")
    print('--------------------------------------')
    
    # If new R^2 the max, store it for reference
    if metrics.explained_variance_score(y_train,y_pred) > max_r2:
        max_r2 = metrics.explained_variance_score(y_train,y_pred)
        max_n_trees = n_trees
        max_rfr_sem = rfr_sem
        y_pred_rfr= y_pred
        
        # Store Standard Error
        se_rfr = stats.sem(y_pred_rfr)

# Return max R^2 and corresponding amount of trees in forest
print ('Max R^2 is: %0.5f' %max_r2, 'at', max_n_trees, 'trees')

!pip install tabletext

import tabletext

data = [['Models','Train. MSE','Eval. MSE','Eval. Ratio','fitting Time (in s)'],
        ['Linear Regression model',0.00155,0.00144,1,0.028],
        ['Xgboost',0.00091,0.00083,0.50,1.163],['Gradient Boosting Regression model',0.00078,0.00046,0.319,1.222],
        ['Lasso Regression model',0.00216,0.00208,1.44,0.197],['Random Forest Model',0.00073,0.00009,0.0625,10.893]
        ]

print("Linear regression will act as the baseline for model comparison.\n The evaluation ratio of each \
model is equal to its evaluation MSE divide to the \
evaluation MSE of Linear regression. \nThe smaller \
evaluation ratio, the higher accuracy of model’s \
prediction.\n")
print( tabletext.to_text(data))

